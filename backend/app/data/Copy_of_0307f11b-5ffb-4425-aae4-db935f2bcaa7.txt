Speaker A:
Okay, there we go.
Speaker B:
So I think, as you probably know, we are here today just to learn more about web testing in the MDN Web DNA 2019 and 2020, web testing was the number four most frustrating aspect that developers reported. Judging by your reaction there, I have some thoughts to share. One of the things that I would love to do today is have you join this session to map out what your developer process looks like and within that when testing becomes a factor. So in chat, I'm going to share a link.
Speaker A:
Okay.
Speaker B:
If you could go ahead and open that up when you get a chance. And then I'm going to share my screen so that John can follow along with what we're doing.
Speaker A:
Oh, good old OS X. Believe I have open. Yep.
Speaker B:
Are you seeing the testing workflow with the horizontal line?
Speaker A:
Little add event button too? Yeah.
Speaker B:
So how this works is you can add events and I'd love for you just to map out what your process looks like as a developer. And for right now, the vertical axis doesn't matter. You can place them anywhere you want along the vertical axis as you add events, they're actually going to stack on top of each other in the middle, but you can drag them left or right to arrange them into the workflow that you like.
Speaker A:
Are you looking more for what my desired workflow is or what my current workflow is with my existing team, or what maybe I've done in the past that's worked best?
Speaker B:
Why don't we do what your current workflow is with your existing team that out? I'd love to know what you would desire.
Speaker A:
Okay, and are you kind of looking for like where it goes from like we're committing code to the repo to we kick off like automation jobs or with our like CI pipeline or like what kind of detail or clarity are you looking for? These.
Speaker B:
I think the more detail we can get, the better. It just helps us understand where the different steps are and what.
Speaker A:
Okay, so funny story is we kind of don't have one right now, which is a current problem that I have. I don't want to say unfortunately, but it's not. I mean, what, you probably hear this a lot, but testing isn't something a lot of, at least in my experience, a lot of engineers want to deal with, especially when it comes to automation. So finding volunteers to do that can sometimes be a stretch. But. So that is something that is on my plate right now. However, we almost have nothing, but we have systems that allow us to potentially do the testing, but we currently don't really utilize them. Just to set context to it.
Speaker B:
And what is CI stand for?
Speaker A:
Continuous Integration. Okay, I can be more explicit in that.
Speaker B:
That's okay. The abbreviation is fine as long as we have the recording accompany it.
Speaker A:
So, yeah, to try to walk you through this as best as I can. So currently we're pretty simple in what we do because of a number of reasons I could spend hours getting into. But basically what happens is we develop locally on our machines. We have the ability to run the application locally and as an engineer should be testing locally. And testing is more like manual QA yourself against your own local environment. You feel your code's done and ready. You open a pull request, you are required to get a peer review of the pull request, which requires one approval at the same time, a CI job. And this CI job runs at every stage essentially, or can run at every stage, but the CI job runs against the open pull request. So the branch that it exists on, it will do two simple things of more or less running our build and slash unit tests. They're kind of one in the same. So the build consists of a very slim amount of unit tests. We don't currently write unit tests. So there's like three tests that might exist from when it was like a proof of concept and whatnot, but that is when those would run. We just don't really have any. And we haven't been creating them out of pure means of speed. After everything gets green on a pull request. So it means you have an approval and the continuous integration jobs are happy. You then merge that code into the master branch. From there, master auto deploys to our dev environment. The dev environment is just like an existing or an always existing environment that people can go to look at the application as a whole without being locally. But in the idea that this is more or less running in the configured space of how it would be as a full application afterwards. We create manual releases at a cadence of it used to be daily. We have since dropped that to as needed. We basically merge everything into the staging environment that was in dev at a particular cutoff point. And then that gives an environment that is more or less volatile than say dev is. And that's where we have some QA folk do manual QA against that environment. And then on approval from that, we do manual releases to a pre prod, which this sometimes happens and doesn't always happen, but we have the ability to have a pre prod where it's not always available to everyone. Like we set a certain amount of people, a certain percentage, if you will available to it. But overall, I would say more often than not, it's not occurring anymore. We kind of just go straight to produce and then we deploy to prod and that's kind of it. Now our ideal world, and I can add some additional things, is we would have more automated end to end tests and we would be using the Cypress framework with another tool called applitools for screenshots and do comparisons that way. And this would likely occur at multiple state. It would occur at this stage and then if I could just add the same event. So we would want it to occur. This is of course not currently happening, but we would want it to occur at multiple stages throughout the process to ensure that we weren't breaking anything. This would become one of those continuous integration checks and even to the point of doing it again after deployment or some variation of that, after we do deployments to ensure that what made it into production is what what we actually wanted and do checks against that. In terms of the amount of testing that happens here, this is probably very. That happens at local. Sorry, I'm pointing on my mouse and realizing it's not showing up there. But when it comes to like local development, it is like the bare minimum of like this. I loaded it in my browser from my local dev box or whatnot and it works. And then I create my pull request. I don't think we're going as in depth as we should or because of the fact that we're more just pushing code to get it in right now. And the vast majority of the project that we're working on right now is not released to the public. So we're kind of just rapidly developing towards a, a timeline that isn't quite realistic in that sense. And we don't have the bandwidth to adopt some of these practices like we want to. But so like in terms of we're not doing a really good job at responsive in term for web and making sure that, you know, it works in the breakpoints that we've enabled. We do develop it for it, but I don't think we're being as comprehensive as we could be. I know we're not doing consistent cross browser testing either. So it's usually whatever browser you're using as an engineer. So Chrome is usually heavily covered. Safari can sometimes be covered because one of our engineers just happens to prefer Safari. And then in terms of Firefox, I don't know that we do any explicit testing on that browser, but those would be the three that we're supporting and we're Just assuming that going with Chrome at this point, now that Microsoft has adopted Chromium, we're not doing explicit testing on that specific browser. So we don't do a lot of that. I can't speak to what QA does when it gets to the staging environment. I know there's some amount of making sure from an acceptance criteria perspective that that's done and there's some design fidelity checks, but we currently haven't looped into our feedback loop actual design and actual product to make sure that we're doing what they're asking of us. So we're trying to figure out how we can get that in terms of a QA process. But basically the big piece that we'll probably rely on the most is trying to integrate these automated end to end tests. And our philosophy would be using a framework called Cypress and I guess a SaaS service called Applitools for screenshot testing and basically do behaviorally driven end to end testing from that perspective and using screenshots, taking screenshots of the different places where we want to do validation rather than relying on data, and so using that to both cover what a unit test might more historically have covered for front end. And again, this is very more front end focused. I should have prefaced that as a front end engineer. So we would focus this a lot more towards the front end tests and less on unit tests. In terms of I worked in a lot of places where you've seen the traditional React's the dominant framework for rendering applications these days and you'll see a lot of the unit tests say, I sent this data to this React component. I expected React to output like this. In my opinion, that doesn't provide a lot of confidence in the code that you wrote because you're just saying, did React do what React was supposed to do versus if you get a screenshot, you can then cover your styling and your css. You can cover the layout, you can cover the user experience from a behavioral perspective and you can do that all with that single suite of tools that at that point a unit test isn't giving you as much confidence as that screenshot test would give you. So the ideal goal is to utilize Cypress with applitools for screenshot compare as much as we can and to unit test where it makes sense from maybe a more service or logic foundation, but less from. I want to ensure that my page or my component looks like this. That can be handled from a screenshot perspective and from the concept of like. That is what the user is going to see. So if you're screenshotting a browser, you're more or less screenshotting what the user experience will be versus you could have a passing unit test, but from an integration perspective or a behavior perspective, you get into a state that the unit test didn't account for and you now have a bad user experience. So that's kind of where the goal from that focusing on the Cypress and applitools setup would be. I know I rambled a lot, so please feel free.
Speaker B:
No, it was perfect. It was very thorough. I appreciate that. I'm curious, so the automated end to end testing using Cypress, you mentioned that that would be your ideal workflow. What do you think has stopped your organization from building that into the process right now?
Speaker A:
So the organization as a whole has that process available to other teams. I just happen to be on a very brand new team that is of people who have never worked for the organization before. So just not aware of what's available to us and figuring out who to talk to get that kind of integration made available to us. Applitools is a service that costs money. So it's not like a free thing that we can just install and run locally or something to that degree. Where Cypress is, Cypress does have a software as a service kind of plan that we can use, but from a coding framework perspective, we can use Cypress and we kind of planned to use that ahead of time. And we're going to go a different route until we learned that applitools was available to us. But the team that manages access to that account has been more or less repurposed to build something else. So they're hard to contact and get support from to either get access to the software or otherwise. And so it just hasn't been a priority from us from a roadmap in terms of where we're spending our engineering efforts to try to kick this project off. So it's basically just internal organizational, I don't want to say failures, but some people have access to it, but the whole team doesn't have access to it. And so getting the whole team access to it has yet to happen. To actually utilize it then even with these two tools, we still have to develop the strategy and how we want to apply the tools and whether or not there's any additional internal frameworks that we want to develop for easing of using the two tools that might be more explicit to our experience where without diving too much into say, Cypress as a framework, we can create certain behaviors that can then be shared. So it's like okay, every time I need to make sure I'm a logged in customer in this flow, I just can trigger this function. But we have to build all of that out and determine how we want to build it out and how we want to share it and things of that nature. So kind of just having it be a priority and then having the bandwidth to adopt it further is probably our biggest blocker as from an organ from my team within this larger organization and then a lot of it. As I think I was mentioning more to John, I'm one of two full time engineers on the team and the other 20 to 30 engineers are all contractors, so they aren't of the organization. So there's a little bit of disconnect in what is available and how to ensure that everyone adopts it because of some of that as well.
Speaker B:
Gotcha. That's a lot of contractors to work with.
Speaker A:
I was the first full time hire engineer for the team. The project kicked off purely from contractors when I started. So and then since I've started I've there are only two other full time organizational individuals as part of the team, which was the SVP of engineering and a program manager. And the program manager was the only one who had historical knowledge of the organization. Myself and the SVP were brand new to the organization, so knowing what was available and the way things worked or operated at the organization or the company I work for was limited. And so as we've grown, it's just been more and more new people who don't have necessarily context to what may or may not be available to us from a tooling perspective or what's the standard of practice or what do other teams do and so forth. And so there's been a lot of slow to learn from that side of the house as well.
Speaker B:
Mm. Out of curiosity, when you do have the opportunity to more systematically implement end to end testing with Cypress and Applitone, what do you think that will or how do you think that will change this process and what are you like most looking forward to or most dreading about that happening?
Speaker A:
Yeah, so I think, I think right now it's very, our process is very shoot from the hip and hope for the best where this will provide. Like once we start to adopt it, it'll provide a lot of accountability and a lot of exposure to stakeholders. Like as I kind of mentioned before, I really like to see feedback from design and product start coming into the loop because we're kind of just pushing stuff without any like did we actually do what you guys wanted us to do, like, we haven't gotten that far yet, but this would allow us to work with them to develop, like, what are the user flows that are most important to you? What do you need to ensure that is working and things of that nature and then expose that to everyone. So it might slow the process down a little bit, but it'll at least increase the confidence of what we're shipping. Whether or not it's like, we can get from an accountability standpoint, we can get okay on if something is caught by these test suites. From whether it's like a branding style change or we changed the layout or we changed the flow of something and that's what was desired. We can get sign off on like, yes, this is what we wanted or that's an acceptable difference or change or like, yeah, that might not be what we wanted, but it's okay because product said it was okay and gave it the green thumb. And it can move through our pipeline to the next stages and just be more aware of where our gaps are and what we should be shipping versus what we do ship, if that makes sense.
Speaker B:
One of the things I want to do is put an overlay on this timeline or process that you've just mapped out. You leave the automated ed where they are because we know that is hypothetically what you'll hopefully have in the future. But for all other pieces of your process that you've mapped out, I'd love to know. So this is where the vertical axis comes into place, where everything is stemming from the middle there is neutral, and then above that becomes frustrating and below that is less frustrating. So I'd love to see where you would map these different milestones of your process amongst a frustrating scale.
Speaker A:
Sure. So I guess I want to put context behind this, that this is more frustrating because of where my current organization is at, but it's not necessarily been a historically frustrating part for me at other organizations. Is that fair to note? Yes, absolutely. Okay. So this, right now, we are not doing a good job of this at all. And I'm putting this up here because we're just blindly approving code to put in there. This can sometimes be frustrating. That's neutral because it can either work well or work not. So some of these, because they're manual, they're frustrating and I'd like to see them more automated. So a lot of the manual processes I come. The current organization I met is not this organization, but I've worked at Amazon before and Amazon has a very mature development tool environment for their engineers. And that's because Amazon's as old as it is and it's a very tech oriented company and money is just oozing out of their ears so they could just throw a bunch of engineers at the problem essentially. So some of this also stems from my more or less familiarity with how nice I've seen certain of these processes occur in Amazon specifically or probably in general like one of your faang companies versus where I'm at now where there's not a lot of that. And so because of some of this we don't have these automated things that we can make manual and interrupt. But for us as a team, it's hard for us to get support from the other more DevOps related teams that help drive some of that automation process. So even if we did have like the automated testing, like where how we can integrate that into this process so that we're getting like a thumbs up or thumbs down from our automation to just if it's green, it just kind of moves to the next stage or as far as we want it to move without human interaction. And we don't have any of that right now. It all kind of requires a lot of manual involvement and we don't have good processes around ensuring that we've adequately tested in those particular environments because again, most of it's either manual from just someone's browsing the application in a browser and I don't know if that is also being done in multiple browsers and things of that nature. And some of the using Cypress would account for both like browser breakpoint and otherwise kind of testing that we just don't have yet.
Speaker B:
And then the testing, the second milestone there, testing QA against local environment. Are there any additional pain points around that that's causing the frustration that we haven't talked about?
Speaker A:
So for me in terms of the testing workflow, a lot of this is very much pain points because of just organizational, what's the word? I want to look for organizational problems. And so for me the problem that I have with the manual QA is that a, it's from a team that's 12 and a half hours ahead. So there's a big delay in terms of getting feedback and then the way the feedback is presented is through a spreadsheet instead of filing bug issues that we can then prioritize. And then because that's not the case, we have no prioritization of these bugs. We have no way to validate whether they are bugs. If this is an edge case or is it an actual bug or working even with product to say, hey, is this an acceptable bug or do we need to fix this in terms of priority and stuff like that? We don't have any processes around that. I'm also very disconnected from that particular process at the moment. We have someone who's in the Europe time zone right now, does a lot of work with the India time zone contractors before I'm even awake. And a lot of that process is just worked out internally. But it's very much the feedback loop is within engineering and I don't think we're getting enough exposure to the actual stakeholders for some of it. So there's a lot of frustration in there and it's not clear as to there's no test plan. There's no clear concept of what is actually being tested either.
Speaker B:
Gotcha. Okay, so with the end to end testing using Cypress and applitools, I'd love to know what why those tools like why those might make the cut in terms of what you want to use when you do deploy this type of testing.
Speaker A:
Sure. So Cypress has been a good tool that I've used in the past with other screenshot testing tools. But Cypress makes it relatively easy as an engineer from a syntax point of view to some degree of how you build the tests in terms of how you're and because it's running it in a browser essentially in the background, excuse me, it's easier to interact with or to the tooling and whatnot from the framework that they provide enables you to more easily interact with your application and get pieces of information out of it that there's additional tools that if you really dive into Cypress full fledged they even have it where you can see the test operating and performing the actions that you're wanting it to perform. Like I want it to click on this, I want the user to scroll this far kinds of things so you can actually see that being performed in real time if you want to run them locally. So there's a little bit of nuance that that can provide and just in terms of like some of the more traditional frameworks to achieve this can be a little bit more heavy handed versus the helpers and functions that Cypress has. It's also well documented. It seems to have a lot of community driven support around it too in terms of just recommendations in general. And then from an applitools perspective that is more because the organization at a large has accounts or licensing for that. And I don't have explicit experience with applitools but from reading about it it seems to be the one of the top tier and for the price of it, the top tier kind of screenshot or compare kind of tools out there and how it's driven by their AI and machine learning processes. If applitools wasn't available, there were other screenshot kind of services that we were going to recommend. One of them was going to be, I'm drawing a blank on it. There's one that used to pair real well with Cypress that they partnered with in terms of blog posting and whatnot. Drawing a blank on it.
Speaker B:
It wasn't Percy, was it?
Speaker A:
Yes. Thank you, Percy. I knew it was a P. I kept thinking Puppeteer, but no, Percy IO was kind of. The plan was to say we'd use Cypress and associated with Percy and use their. Integrate with their services and use their services to surface any kind of issues or errors in our continuous integration through GitHub and whatnot and then take you to Percy to review and say, yep, this is a good or I'm taking a new baseline or this is acceptable or this isn't acceptable, things of that nature. But applitools existed and that kind of fits the same, fills the same features that Percy would have and then some and what the organization is already using. Then the fallback to if something like that wasn't available and we wanted to do a little bit more of it hands on ourselves, was to use Cypress with Puppeteer and use Puppeteer as our headless browser experience to deal with taking screenshots and otherwise then if none of that worked, go all the way to jest as a testing framework with jest snapshots against Puppeteer. But because there seemed to already be internal adoption of Cypress at the company itself, as well as the availability of applitools from the company, it just made sense to kind of go down that route. And the team more or less was in agreement that if those were going to be the tools we used or if that was the route we were going to use for testing, that those were good tools to attempt to adopt.
Speaker B:
Gotcha. What was I going to ask? Just totally drew a blank. We'll move on to a different question. A lot of the questions that I have, I feel like you've already answered about like how big your test suite is, how much time and effort you spend on it. But it, you know, based on what you've mapped out here and some of the frustrations you've explained is that the testing environment in your current job is just might like or have experienced in the past. That seem fair?
Speaker A:
Yeah, it's the team is so new and are in terms of the team is relatively new to the organization as like it didn't exist before and then it existed long enough before even I started as just all contractors that it's been there's a combination of slow to adopt because it's a change in the way that's currently working right. Like this would introduce a new step in the process that a lot of people just want to be able to continue shipping code and then in addition to adding that new step is having the bandwidth to create an environment that would for engineers and stakeholders to get not only confidence but to have use of this in a manner of which it doesn't completely slow down or halt the whole process of getting code out there. And so it hasn't from an engineering leadership perspective it's always been a priority but from the realistic expectation of delivering the product it then becomes less of a priority in terms of ensuring that we're adopting these testing practices. And now the longer we go without it, the harder it is for people to jump on board and be like okay, yep, this thing that I used to push out in half a day is now going to take me two plus days to push out in terms of the added effort or gates that occur because of it. While that will ensure a better end product, that creates a lot of friction for the for some of our engineers too and myself included.
Speaker B:
Gotcha. I remember what I was going to ask now. So you had met with Cypress and some of the other tools like kind of adoption within the industry and documentation and support like how do you stay abreast of changes to available tools or processes around testing?
Speaker A:
So the it's funny that you mentioned that but the best that I personally do is every once in a while I'll either I'm a Redditor so I'll scroll through Reddit and sometimes there'll be a headline in one of the more web related things that I encounter that will pique my interest. But I also am subscribed to JavaScript weekly explicitly and then there's enough I've clicked on enough Medium blog articles from just doing googling of like how do I fix this? And it takes you to a medium article that now the Medium digest will through the good old cookie tracking of me as an individual has now sometimes provides a headline here or there that I will occasionally click on. And ironically one of those more recently was a comparison of using what's the new Microsoft framework that I'm drawing a blank on what it was in the article, but it was comparing four frameworks with Cypress being one of them in terms of like speed and use and the blog article seems very much focused on trying to sell Cypress even though Cyprus seems to be the worst performing of them but the from a headless browser perspective but it was it was like doing puppeteer Cypress web drivers with Selenium and something else and then there's this new Microsoft framework that I hadn't heard about that I then started kind of doing some brief reading about that is similar to that for in terms of also provides cross browser testing in it from what briefly I was reading about but so I would say like the main sources is occasionally any headlines that draw my attention between browsing Reddit and a couple email subscription things is how I keep up with it or when you're trying to solve a problem and you can't figure out how to do it in the current framework and then the good old you go down the Google black hole of oh this is how everyone else seems to be doing it now maybe we should adopt this kind of thing makes sense.
Speaker B:
We've talked quite a bit about end to end testing and you've talked a bit about screenshot testing. I'm wondering if there are other tests that you don't do specifically and why.
Speaker A:
Yeah so I mean I kind of touched on the concept I have or that concept the philosophy I have when it comes to unit tests, especially for front end development. I think it's who is it? Kent. Kent Dobbs. He's a pretty big voice in terms of testing in general in the engineering world and I think he has some I think one of the quotes paraphrasing obviously is something along the lines of you should write tests and the vast majority of those tests should be integration tests. And when we say end to end or whatnot, I tend to think of integration tests and end to end tests as more or less the same thing. You're testing the behavior of a user and you're following them through your application as best you can and that's where the screenshots come into place. And so a lot of those tests would replace or would cover the same thing that a unit test might cover in a more in the render portion of your front end application. And where I find unit tests to be a lot more prevalent and a lot more useful in terms of confidence is typically on more of that API layer or that service layer. I find them to be a lot more important at that point and even to some degree those can be replaced by an additional layer of what I would call truly integration tests, which is where you're actually testing the data contract and you're making the call over the wire between. My application made a call to this API and I expect this response based off of this data that I sent them. That kind of integration testing is important and a lot of that can sometimes also give you the same level of confidence from your API that unit tests might have. But I think there's a lot more benefit in that more back endy world for having unit test coverage and being confident in Those services and APIs that you're delivering over the front end side of your application being more covered by the screenshots in terms of you get both because unit tests don't test what the user is actually seeing, they're just testing how the application is working. And so testing what the user is seeing. You get both the style benefit and the functional benefit from that perspective where there is nothing to see other than a data payload from an API. And so that's where like unit tests can come into play. There more in terms of other testing that we don't explicitly do now or won't necessarily do in the future. I don't know how far the project that we're working on right now spans every device, so connected tv, mobile devices, tablet devices. And so I don't have a lot of insight on what the plans for testing from that perspective are. Most of mine is more explicit to web, but I do know that there's similar desires to use applitools I think provide support for some of that. But outside of that, in terms of like cross browser or cross device compatibility, there might be a very slim list of what we end up defining as say our service level agreement of what we support. So being comprehensive from that perspective, we probably wouldn't be.
Speaker B:
Gotcha. And then you haven't mentioned it. So I'm going to just ask your reaction is what about accessibility testing?
Speaker A:
Yes, so I would say that is unfortunately not been discussed or prioritized. However, I do think it's something that we will end up being like, oh wait, we didn't do any of this and we'll have to go back and, and start to do it. There is an amount of what we're working on of a sense we're kind of like a media company. So there are parts of accessibility that are kind of a requirement that we have to do in terms of we have to make sure that subtitles, for instance, if subtitles are available, we are capable of presenting those subtitles in terms of media Playback. But like from the perspective of the individual apps being accessible from the standards that are set out and say even going so far as to go from a colorblind perspective from the branding and styles that are picked, there's no explicit direction and there's no explicit testing to ensure that that any of that is occurring currently. But I do know that there is a limited amount of in terms of the website of things where we develop. Like if there's a particular element that we know should have like maybe an aria label or image tags having the alt tag, there's some amount of that that's just inherently kind of done by us, but it's not due to any accountability that we should be imposing on ourselves. So I'm sure just based off of the project in general that it'll end up cropping up, but currently none of it is occurring in any direct manner that I'm aware of.
Speaker B:
Okay, and then one more is performance testing.
Speaker A:
Yes. So I did kind of leave that out. We don't have any pipeline in terms of facilitating that performance testing from the front end perspective. We do run Lighthouse occasionally and we have numbers that we report on, but we haven't been reporting on those lately in terms of like page load times. But I would like to see us go even beyond that and have more metrics and alarming and things of that nature driven towards even our latencies and things of that nature when communicating with our APIs and not just focus on page speed. But I also don't know our team is very much the render portion of our application unless the back end services that drive all the data. So I don't know what level of performance testing is done or available from the back end side of things to test our different APIs and whatnot. But I would like to see us at least drive metrics on those so that we can know. And then outside of that I would like to see our in terms of when we released or prior to releasing to production running, even if it's as limited as Google Lighthouse or Chrome Lighthouse. I'm not sure how it's branded, but running that against the application to give us insight to that itself. But there's no again there's no feedback or there's no pipeline or loop regarding performance testing. It's more a very kind of as we feel needed basis. Currently.
Speaker B:
Within that as you feel needed basis, are there any particular browsers that you focus on with performance testing when you do do it?
Speaker A:
For right now it's just Chrome and Mobile Chrome from my understanding okay. In terms of page speed, that's the baseline that we use for that. I don't know if we do performance in other browsers currently, I'm about 95% confidence. Say we don't outside of. Outside of whatever the Lighthouse tool operates in, so.
Speaker B:
Gotcha. Well, there's only two minutes left and I don't want to take up more time of yours than we asked for. Are there things that we haven't talked about specific to testing that you think that we should have talked about, or knowing that this is your opportunity to air any grievances you might have to have Sharon, have shared with Mozilla and some other sponsors behind this work, Are there?
Speaker A:
Sure. Yeah. I know personally, like I started development back when IE6 was still a thing. So where I remember the most headache about, and to be frank, like testing has the last couple of organizations I've worked at testing has been we have users, there are qa. To be frank, it hasn't been a high thing. But so to think back to the headaches of what browser compatibility testing was over a decade ago to what it is now, more often than not you can be pretty confident by looking at one browser or another, and there might be some minor styling differences, but nothing to the degree that it was way back when. And so there's. Or where, you know, even five years ago or seven years ago, where you still had like a class for every browser so that you could pinpoint and target styling for all the different browsers because they were all just different enough that you did need to make adjustments. But it seems like I haven't felt like I've been encountering that as much. And even from the way JavaScript is architecture or architected, the way JavaScript is executed on a per browser basis isn't as bad, especially since Microsoft finally adopted Chromium. There was still some issues back in the day with IE11 and even Edge, but since that's migrated away again, a lot of those have fallen to the wayside. And in terms of modern browser adoption from the public, you don't have 15% of your user base still using IE8 or IE9 or otherwise. So for me, I think a lot of my personal pain points in the past were always related to browser compatibility. And now I think more of the pain points are organizationally and getting adoption of what technologies to use and what's the best technology to use. I think if I had, and I know we're a little over, my biggest gripe is that the ecosystem has grown so broad that there's. Which is good and bad. There's almost too much choice. So in terms of some of them and eventually become like the dominant ones in terms of the community. But there always seems to be some new testing framework or some new testing solution or something, and it's like, okay, now we have to decide do we migrate to this whole new platform to try to increase our confidence in what we're currently doing, or do we just stick with what we have kind of thing in terms of. I think I've written tests in at least six different frameworks over the last decade. And while the syntax is mostly the same, the nuances between each one is just a lot of extra lift. Versus for more at the front end where like testing frameworks for say like Java or C, more or less, you have one and that's what it is and it's just improved over time. Versus the web is very fractured to a degree. That would probably be my biggest right as of now. How to solve that is a challenge, I would say.
Speaker B:
Awesome. Well, this has been really helpful. I really enjoyed learning from you, John, Any questions?
Speaker A:
I don't think so. I think I just learned a whole lot about testing, which is awesome.
Speaker B:
Cool. Well, thanks again for joining us today. I'm sorry the technical issues at the beginning, but I overcame them and really appreciate your perspective.
Speaker A:
Awesome. I hope you can find it useful. So.
Speaker B:
Absolutely.
Speaker A:
Cool. Well, thank you for the time.
Speaker B:
Thank you. Have a good rest of the day.
Speaker A:
You too.