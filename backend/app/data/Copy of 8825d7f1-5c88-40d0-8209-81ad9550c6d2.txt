Speaker A:
Ask me again. That's totally fine, thank you, I appreciate that.
Speaker B:
So started. Do you mind just telling me a little bit about yourself as a web developer?
Speaker A:
I'm well what I like to say, seasoned web develop web developer. I started to do web at the end of 90s in 2000, basically in the 21st century. I moved almost exclusively to web development on server side and on front end side. Front end side is my focus but I still do, I mean a lot of work on the backend as well. So that's basically my background. I participate in open source so I mean I'm interested in all this stuff not only professionally as my day job but as my hobby as well.
Speaker B:
Gotcha. Very cool. And what do you do for your day job?
Speaker A:
Right now I am a principal software engineer at a company called Daynata. I lead a small team of developers to create user facing websites. We have like about 200 of them so I mean everything goes into play. How to serve them effectively, how to make sure that user experience is totally frictionless, how we can support multiple browsers because I mean our users, I mean we cannot mandate them in what browser they use. We don't support the Internet Explorer finally, but that's about it.
Speaker B:
How are you finally able to move away from supporting Internet Explorer?
Speaker A:
It turned out that we have less than 1% of users on Internet Explorer so we decided to just cut it because the expense to support it was unproportionally high. Right, so it was money decision, I mean not technical issue.
Speaker B:
The there was somebody I spoke with a couple days ago who unfortunately still has to support Internet Explorer. So he's, he experienced quite a bit of pain around that.
Speaker A:
It is, yeah, I mean it's probably the worst browser I mean there is right now. Typically it comes from. Well we are international company, I mean literally covering the whole globe. So I mean I can tell that usually these people come from developing countries from people, I mean who work in for some states because I mean that's standard, still standard. I mean the browser for them internally. So I mean that's kind of stuff but I mean majority of regular people across the globe, I mean they use it in Chrome, Firefox, I mean Brave, anything else but even H, I mean it's still Microsoft product but I mean it's much, much, much better and it's based on chromium.
Speaker B:
Well, I would love to do an activity with you where I have you map out what your typical workflow looks like for a project or maybe like a day in the life of your Work and then once we have that mapped out, I might have some follow up questions for you more specifically about testing. So I just put a link into chat and if you're able to access that, it'll bring us to a session where we can use a tool to map out the workflow.
Speaker A:
Should I open it in the same browser? How do you want it to open it? I can open it right now. I'm using Chrome, I can open anything else. Firefox, Break, whatever. Chrome should work just fine in the same session. That's fine, right?
Speaker B:
Yeah.
Speaker A:
Okay. Okay. Opening.
Speaker B:
Okay, so in a minute you should see a horizontal line on the page and that just represents time. So basically from the left to the right is beginning to end and you should have the ability to add events to this timeline here. And so I'd love it if you could just start mapping out what your web development process looks like for your work and maybe how you collaborate with your team and so you can, you can keep it pretty. I'll let you define where you want to start.
Speaker A:
Well, okay. I mean typically I'm getting a ticket, source of ticket. I mean we can discuss as well because I mean some tickets come from our clients, some tickets come internally. The business side, the product side evaluate these tickets and they define priority. And after that, I mean these tickets are packaged into Sprints and we do a Sprint Sprint typically two weeks. So I mean that's usually how I mean it works by the time I'm getting ticket. Usually it was discussed during the refinement meetings during, I mean some other meetings. I mean usually it's not like, I mean like something totally new I have to do. In some cases, yes, it can be a research ticket to do research on, say support of certain features across browsers or to do research how we can use them in our backend to implement this feature if we need anything else. I mean something like that. So at the end of the day it goes to a developer, developer looked at this stuff and tries to implement according to description of ticket, our tools. First of all, I mean we have, I mean different, we have different properties. Like our internal tool is made with React and whole infrastructure. The backend is essentially aws. So these are major technical things we're dealing with. I mean for our internal tool, our product facing application is pretty old and we are about to rewrite it. It's based on angular JS, so called angular 1. It's very old, it's unsupported anymore. So it's really difficult to even get some information about it because it started to decay. You go to a webpage with supposed to host documentation and it's 404. So I mean some sometimes I mean you get documentation for a different version. Problem with Angular JS that versions very different especially the difference between 1 and 2 is huge. And I believe the current version is 10. So you can appreciate I mean how legacy this stuff is. What else it does use AWS infrastructure. It uses our proprietary servers. It used some Lambdas Lambdas. All our lambdas save for one is written in JavaScript. The only non JavaScript lambda is written in Go. So basically that's our interest. Now interest is in documentation on JavaScript, maybe go, not so much on other stuff we use like how to do certain AWS and how to do certain React or Angular js. And so that's basically a daily lookup we do. I mean when we develop feature whatever specified by it. What else if something I mean is difficult and we want to have a meeting about that or we want to have a session with like a pair programming with junior developers or pair debugging which is pretty common as well. We because I mean of company decision. Currently we use Microsoft tools specifically Microsoft Teams. We used to use Zoom, we used to use some Cisco product something didn't like it at all. But I mean right now what we use, I mean Microsoft Teams there is a plugin for Visual Studio code which we mostly use which allows pair programming so rarely I mean but we happen to use it as well. But in most cases Microsoft Teams most important for us, I mean is joint debugging more important than pair programming? There are no any tools. So I mean we use Microsoft Teams sharing screen and like going I mean to a website trying to figure out I mean what's going wrong and whatnot. Partially it's because junior developers are not that versed into debugging tools apparently. I mean it's not enough information on debugging tools, how to use them. And I'm not even talking about specific implementation like how to use it in Chrome, how to use in Firefox. But it turned out that concepts are important for them as well. Like how do you actually what's important to when you assessing the performance of website, for example, I mean if you look into tools you will see I mean immediately that there is a DC element DOM content loaded. There is a load event with everything loaded including images. But realistically we are interested in TTI time to interact. It's in most tools but sometimes it requires certain plugin to be installed and it's kind of varied. So all these explanations, I mean how it goes and what's important, when and how, what to look at, how to interpret network section, how to interpret waterfall you have from network and how to interpret the heat map slash waterfall, I mean which shows when browser parses something or paint something and whatnot. I mean it's apparently really advanced concept. So I mean we spent a lot of time talking about those. So people, I mean know what to look for and when to look for and how to look for it. So I mean they're the stuff and when it's done, we're supposed to give it away to a QA team. Problem is that because of the current situation we don't have a QA team. So we ourselves supposed to do testing. We used to have QA engineers who created application written with a pytest if I'm not mistaken. So using Python internally, it uses Selenium that uses, I forgot the company name, I mean the biggest one which provides Selenium API for browsers. So that's how it's done. Obviously as developers, usually we test our code, but literally on one browser. But because we're talking about user facing websites, we need to test a lot of other browsers, including mobile browsers. According to our statistics, half of our users are on mobile browsers. And it's especially so if you go into developing countries like you go to say India. Outside of major cities, nobody, literally nobody has a desktop or laptop. Everybody accesses Internet exclusively, using them in some really, really, really cheap, low memory, extremely low cpu, Android phone. And that's our target. So I mean, same goes, I mean for Africa, same goes for China to certain degree we need to test it. For my personal test for my personal open source projects, if one of my targets is a browser, I usually use a Puppeteer. There is a Puppeteer version. The original Puppeteer version was for Chrome and that's what I usually use. Sometimes I use a Puppeteer for Firefox. In any case, I mean we are really, really interested in testing our stuff in browsers in automated fashion, hopefully without windowless. I mean basically we don't want, I mean we don't need, I mean actual Windows to pop up because I mean we want to run this stuff on servers as a part of CI in totally automated fashion. But so far, I mean, well, I was able, I mean to do it, I mean to set it up on GitHub, I mean for my personal projects. But at the company we don't do it yet. I mean, and it should be. Should be a push to do at some point.
Speaker B:
So do you feel that the steps are accurately captured here?
Speaker A:
Tickets go to Sprint. Sprint. There is a route to developer then develop debug testing. Yeah, I think it is. Maybe it didn't capture communication aspect, but I don't know if you're interested in that or not.
Speaker B:
Yeah, actually you have the ability to add events too. So you can add an event and put it here in the timeline.
Speaker A:
Probably we need to. I don't know how it works like that element. Okay, got it. This one probably goes, I mean about here. And we have communications about diff debug. It goes about here maybe. Well, you obviously you can move them around. Do you like it? So I mean, that's kind of like. I see it. Testing is a whole totally different business. I mean, it's really hard to do sometimes. I mean, we do make mistakes and have to roll back stuff. Everything works fine, but users start to complain. And because, I mean, we deal with money as well. It's really important to do it right.
Speaker B:
Can you say that last part again? You. You deal with, I think money. Money.
Speaker A:
Yeah. Basically Dynata is kind of like company which does. I mean social statistics. Kind of like Nielsen, if you know what I mean. So I mean they do surveys, they do, I mean collect, I mean information for commercial companies, for political campaigns, for literally everything. People, I mean, they haven't sent you for me. If they answer questions, they can earn certain amount of points which at some point can be converted into something tangible like a gift card to Amazon or gift card to something else or even real money. I mean, transfer to PayPal. So I mean, it kind of like brings a totally different dimension. I mean, obviously it's no good. I mean, somebody I don't know, over time, $25 decided to cash them in and they like finished, don't do it obviously. So that's why it's. It's a little bit more than, you know, hurt ego. I mean something else. I mean, it's requires from us. I mean certain. I mean, due diligence. That's why testing is important.
Speaker B:
Yeah. I'm wondering if we could actually talk a little bit more about testing and what types of testing you end up running in this process.
Speaker A:
Well, in again, in my open source project, majority of tests I run, they are unit tests. So the relatively simple they test them in one small feature and I mean, that's it. At my work, we do unit testing as well. Not as much as we wanted, but mostly because my open source stuff, it's still JavaScript and whatnot, but I usually do libraries, I usually do some tooling utilities. So I mean they are quite unit testable. When we talk about generic websites which can be used on multiple form factors with different network speed, with different delays because it's literally across the globe in Australia, it's halfway across the globe. So I mean delay is huge. Even if, if we serve stuff from America, for example. So Unity don't buy us as much for actual websites. Even screenshots don't buy that much. Like I said, because of different form factors, somebody looks at your website a little bit differently and if you have some kind of bug in css, certain feature can be pushed down, up or I mean like vanished completely. So I mean hard to tell. So usually we do high level testing of website. Like there is a scenario user should complete certain set of tasks like answer questions and cash it to some money. So I mean there is like automated program custom obviously written by QA engineers which will do that by simulating clicking buttons, selecting checkboxes and whatnot. So that's kind of testing we're talking about.
Speaker B:
Okay.
Speaker A:
Stuff is pretty fragile to be. To be honest. I mean kind of like fragile. I mean breaks, I mean sometimes, I mean for no reason. But that's the best we can do.
Speaker B:
Are there any types of tests that you don't run?
Speaker A:
No, I. Well, kind of like, I mean the idea of testing we have is we have unit tests for usually for stuff which is not visual, like some algorithms, internal algorithms or whatever, some math. We have high level UI tests for our websites. Idea is, I mean that this unit tests, this high level test run in different environments. Unit test can run, I mean, you know, in node, in browser, whatever. But high level testing is supposed to be run in different browsers with different form factors. That's basically it. We don't do anything else.
Speaker B:
Why don't you do any of the.
Speaker A:
Let me say, I mean that we are just one team in the company obviously. I mean there are some other teams, I mean which probably do their own testing. We don't have a company standard for that. But I mean we are practically the only front end team, front end facing team. Other teams, I mean they probably do, I mean their test, I mean or like unit tests with just data with some simple stuff because they don't run their stuff, I mean in different environments. So it's, I assume it's going to be much, much simpler.
Speaker B:
Gotcha. What would you say is like how would you describe your test suite beside like how many tests you're running.
Speaker A:
For my open source projects it can be hundreds, like 100, 200. That's the common scale, especially for long running open source projects. Because I mean if people, I mean find the bug or propose a feature, usually I start with writing a test for bug feature and then I mean I'm, I'm trying, I mean to make sure that it works at the work we have less unit test, we have maybe less than hundred, maybe multiple dozens, like 50, 60, 70, easily maybe about hundred but not more high level UI. I believe it's about 40 different scenarios I tested like 30 to 50. I mean, I think, I mean the range that's like I said, I mean it was initially done by QA engineers and we kind of like hoping that we will get some QA engineers later. So this is more or less, I mean static thing. I mean we don't change it very often, only if it breaks. So I mean, because of that, I mean I don't do much coding in this area. Don't really remember exact number, But I think 30 to 50 is probably the right number.
Speaker B:
And then could you just describe what you mean by high level UI testing?
Speaker A:
Well, imagine just super simple example. Imagine that you answered a bunch of questions, bunch of surveys and you accumulated 25,000 points. And you're like I want to cash at this point, I want to buy some stuff and it would be nice, I mean to have something. So you go into reward section and you see what rewards are available. For example, I mean you see that, I mean 1,000 points is about $1 and there is a $25Amazon gift card. So you click on this stuff, it goes to shopping cart because I mean potentially it can be like 35,000. So it's going to be 25 gift card and $10 gift card. So you can select both or you can select different variety. For example, I want gift card for this hotel chain or I want to use it as a air miles. I mean depends. So I mean rewards selection is dozens and dozens of choices. So you select this stuff, you click checkout, you will be asked some questions. Finally you confirm your selection submit and it goes away into server. At some point you will receive email with code or if this is a physical gift card, in this case you will receive it by mail. So that can be exactly what I described with email can be a high level UI tester. We put 25,000 points in your in a test account. After that, I mean the test suite goes and like logs in goals to reward, find this Amazon gift card, select Amazon Gift Card, do check out answering questions, submitting stuff and then we check them. Within say five minutes our test email will receive notification about bot gift card. So like that it's really simple, but it's high level. So I mean at low level, I mean what PYTS does is just like literally clicks in on proper buttons and enters them in proper information. That's basically plus weights, I mean, you know, for servers.
Speaker B:
Gotcha. So one of the things I want to do with this timeline that we've mapped out is I'd love to understand where in this process you experience the most frustration. So the line that we've just mapped everything out on is neutral. And then anything above it is what causes frustration. Anything below it does not cause frustration. So you have, you can just drag them up or down. And I'd love to just kind of dig in a little bit more on the aspects that are frustrating.
Speaker A:
You know, to be honest, everything is frustrating, but I mean to different degrees. Tickets are frustrating. Yeah, kind of like that. Sprint. Yeah, Sprint. Yeah, probably like that. Communications again, probably like Sprint to developer a little bit more develop. Yeah, Debugging even more frustrating like that. Maybe that one is probably about. Yeah, only because I mean we don't do it as much as like individual debugging. Testing especially right now is very frustrating because unity test maybe frustrating but problem with unit test, I mean they don't help us much. Yes, I mean we do have some algorithms, I mean they written, they were tested and we do not modify them. I mean very often it rarely happens because I mean, you know, business logic, I mean for these algorithms usually, I mean certain stone high level testing is probably here, let's put it over. I don't know, maybe it's our fault because I mean, you know, we need to spend more time setting up automated testing. We don't have automated testing for multiple reasons. Mostly how infrastructure is set up partially because I mean high level UI testing is pretty fragile constantly. I mean it breaks something, I mean it shouldn't, but it does. So I mean you go, you look, you debug and it's some minor thing. I mean something minor change, but for some reason it affects outcome of this algorithm. So kind of like that, I guess.
Speaker B:
Okay, and you mentioned that you don't do a lot of automated testing. What's the reason for that?
Speaker A:
High level UI testing is a suite, it's automated itself, but somebody has to run it. For example, for my open source projects, when I push changes, the test suite mostly unit test Run. I can see for example how it behaves on different node versions or in different browsers, if that was a project for browsers. So I mean that's pretty much automated. If something is wrong, I will receive email. That's how. I mean it works on GitHub and almost everywhere over there because architecture. Well, I'm sorry, take it back. Because infrastructure is kind of like we use them in AWS mostly. But our APIs, they belong to different groups inside our company and they may be not AWS or they may be aws, but we don't have access to them. So because of that, I mean, we cannot retest our stuff automatically. If for example, downstream, some API has changed because we are separated from that team. Same goes. I mean when we change something, I mean we can retest our stuff, but not APIs we depend on that can be like that some. And that one of the big issues, by the way, that frequently for testing we need not just, you know, some code, but some data to prepare specifically for this new feature we were developing or for update or for bug or whatever. So I mean it's not enough for us, you know, to fake data. We have to modify databases, we have to test that it works with other teams code as well. So I mean that's all complicated because of that. I mean, sometimes, I mean we don't have a trigger from one environment to another, as simple as that. So I mean frequently testing is done literally by somebody sending email or chatting us Microsoft Teams guys, we modified this stuff. I mean, can you test? I mean it still works, works with you. So I mean it's more about organizational problem and it's a kind of like a large scale organization, not at the team level, but how to do it between teams, how to notify, how to track dependencies, what should be tested and what happened. So data preparation is a big issue as well. That's why, I mean we're kind of like, I mean not using QA engineers, but QA engineers. I mean not only coded tests, they prepared data for those tests. Now it's our developers not set up to go to databases with some custom tools and do some stuff. So really hard.
Speaker B:
You mentioned a little bit previously about some of the browsers that you have to support or the ones that you don't have to. I'm wondering if you could just elaborate a little bit more on the browsers that you do support and why and then maybe talk about any differences between mobile and desktop.
Speaker A:
Okay. I am in charge of CDF used by the company, so like once a month I go and check what kind of user agents we fielded and we can see that number one is Chrome. And after Chrome we have Firefox, Safari. Safari usually comes from mobile platform and usually from developed rich countries. So we're talking America, maybe Canada, maybe Europe. Outside Safari is negligible. So we need to provide support for all our users. So I mean this is important thing for us. I mean we're doing it on regular basis, assessing our users. We tried to use some, you know, some. There are some industry watchers which publish what browsers are popular, what not. It turned out that their statistics is different from our statistics. So we rely on our statistics. That's basically it. I mean to be honest, if you don't count IE majority of our code just works. We rarely have hiccups that I mean we use certain JavaScript feature, it's not supported on some relatively popular browser. If you not count, I mean that's usually not a problem. Personally, as a source of information on browser features I use MDM and I instructed who my team to use MDN as well because I mean it's nice, you can trust it. And like I mean many other sources that's basically. Usually they have a really good description and really good examples which is important. I mean examples which are inside an article, you can try it yourself. In some cases you can even modify and see how it goes. That's really, really nice. I really like that. Now MDM started to publish compatibility tables as well in all times. Usually I used can I use website which is great by the way. I still use it but if I read about certain feature I really like to have a compatibility table, see how this feature is supported across multiple browsers, desktop and mobile browsers. So I mean that's really important for me as a developer. Sometimes when we propose certain feature we do research and kind of like it goes the foundation of our decision to use it in certain features or I mean use something different. So I mean that's, that's another thing.
Speaker B:
Can you explain that last one a little bit more? What do you use to help you decide what features to implement?
Speaker A:
Okay, for example, you know what CDN is, right?
Speaker B:
I don't.
Speaker A:
It's a content delivery network. Essentially it's a fancy way to say, I mean that it's a system which sends files to your browser. So your browser receives HTML page and inside HTML page there are references to say script and CSS file and maybe some images. So it will request these separate files from a server. Now servers can be Done differently, they can use them in like different type of HTTP protocol. The current most popular one is HTTP 2, but there is HTTP 3 already brewing and probably going to be standard pretty soon. If it's not standard already, I didn't check. HTTP 2 protocol for example allows you to reduce latency which is especially important for international users. We sitting here in America, I mean for us, I mean service really, really fast, try to do it in China, it's first of all it's halfway across the globe. Plus they use them in a great firewall which introduces huge delays. So another thing, I mean how much bytes you want to download? Modern applications, web applications tend to be pretty large. You have basically two sides of the business. One, the more bytes you send to user, the more money you pay. It's as simple as that. Literally everybody, aws, whatever, I mean they charge you by a megabyte or kilobyte but basically by how many? I mean you when receivers another side of a business. Imagine that you are in India, you have a really really cheap Android phone and Your network is 2G. There is a huge difference between them and sending you half megabyte of data and megabyte of data. For us in America it's going to be 20 milliseconds or 40 milliseconds, we don't care. We will not even perceive it as a delay. For them it can be one minute versus two minutes even one minute. Nobody will wait, I mean one minute for websites to load. Trust me, I know. But I mean two minutes, forget about it. Nobody will see your website, I mean if your website loads for two minutes. So I mean there is a concept of cdn. CDN usually does intelligent loaded for example. I mean they use compression instead of sending text file like JavaScript, CSS, HTML uncompressed, they compress it and it's easy savings of two, three times, sometimes even more five times. But if you pre compress this stuff statically, using them in like a maximum possible compression and it will take you, you cannot do it on the fly because it will take you a long time. It's even more savings for you and user. If instead of gzip, which is a common compression, you use broadly even more compression. I mean if you use them in instead of jpeg you use some advanced version of JPEG that you have compression savings. If you use VP instead of having JPEG you can save sometimes 10 times more versus JPEG. And there is a new compression, I mean around Avif I didn't use it, but apparently it's Even better than webp. Problem is webp is supported everywhere but by Safari and ie. And Safari, unlike ie, Safari is a major browser. So UTDN should be smart enough to send this user jpeg, not webp but sent everybody else webp. Same goes for Brodly. Brotli is a new algorithm to replace gzip. Again right now everybody supports but I, everybody. But still there are some really, really bizarre browsers which do not support it. So CDN should be smart enough to check if this browser supports Brotli. We will send Brooklyn. If not, does it support gzip? Oh, it supports gzip. Let's see if it doesn't support gzip, let's send them in like plain text. That's the best we can do. The whole point is not to penalize people with modern browsers and like take advantage of features provided by modern browsers yet still supporting in a seamless fashion all the browsers. So I mean, obviously, I mean this Fisher support kind of like, oh, let's use all images gonna be vp. It actually happened to me. One team content team decided to use webp only and as soon as they published it turned out that Safari doesn't support it. Look at feature support stuff on MDN or can I use it will tell you if you can use Safari or not. And they're like, oh, you know, they forgot about Safari. They knew that I didn't support them. That happened. So that's what I'm talking about. Supportive features can guide them in some design decisions, technical decisions, how we serve stuff, how we call stuff and whatnot. On some browser, certain features do not work and you have to use polyfills. Polyfill will include sometimes, I mean can be pretty big and sometimes they can be pretty buggy. So in some cases it's more prudent not to use this feature work around it instead of try to do polyfill versus non polyfill.
Speaker B:
So some of the things that you just mentioned make me want to ask you about. So we haven't specifically talked about this and I'm curious to see if it rings any bells for you, but performance.
Speaker A:
Testing, oh, this is a huge. We don't do it. Performance testing is. I don't do it for my open source stuff either, but it's a big one. Difference is that people frequently pretend that unit tests can be used for performance system. That's not true unit test. I mean they test certain Fisher and if you call this fish a million types and see, oh, now it's, it's Cooling faster. Yeah, it's something. In reality you have to write special tests which exercise certain path, kind of like high level UI testing. So I mean you want to test certain code paths in your application, in your library, in your framework, whatever you provide. So it requires different style of testing, it requires different way to deal with statistics. It's majority of unit testing tools, they run stuff and they benchmark it. How they benchmark it. They basically see how long it took. Oh, it took 65 milliseconds, fine. Next time you run it's going to be 12 milliseconds. Next time you run it's Going to be 200 milliseconds. Why? Because it depends on many factors. So in reality if you want to carefully calibrate your stuff, you have to run it many, many times, maybe with some kind of delays to see how garbage collector affects stuff, how other processes, if you run them in several threads, I mean effect stuff and whatnot. So this is a huge deal for unit test level performance for browsers. Again, it's a huge deal. We need to first of all create special tests which will emulate certain workflow we expect from user and see how it can be done with different type of network. Modern browsers usually have ability to simulate network delays net network speed. They do throttling different form factor. So I mean all this stuff is important and should be done. We don't do it. But like I said, I mean usually if somebody is doing it, they do it the stupid simple way. We already have unit test, let's see how far they run. No, it's not. I mean it's. It should be done differently. And to be honest, to my best knowledge, we don't do not have any adequate tools for that. Several years ago I had a project that I needed to optimize for perf. I ended up writing a test harness myself. I couldn't find anything. I mean working properly again. I mean with unit test you typically care. Oh, I have 100 unit test and see 89 unit testifying. 11 failed. Let me check what failed. When you do performance testing, there is no failure if it fails. I mean the performance testing cannot be done first of all. And second of all, you, you don't really care about how many tests estimate working because they all working. But you should know, I mean what is like medium running time? What is average running time? What is like width of the interval? Sigma. I mean, so you know that. Okay, I mean this operation runs from say 100 millisecond to 900 millisecond. It means it cannot run 2 minutes, 2 seconds for example because I mean this is our. I mean it's like 99 confidence interval or 95 or whatever. I mean that's again I should specify tools which deal with statistics non existent at least I don't know any Gotcha.
Speaker B:
Well, there's only a few minutes left and I would love to flip this back on you and see if there are things that we haven't talked about about testing that you think would be important for this audience to hear or things that might help alleviate some of the frustration that you experience in testing.
Speaker A:
Well, obviously it's about better tooling in my opinion problem that majority of tools we use, I mean they pretty old they spend more time about doing like DX developer experience which is fine but I mean it's not about testing mostly so I would prefer to them spending more time on testing facilities, testing harnesses, multi browser testing still really hard for example I mean Puppeteer they now produce a version that you can use for Chrome and Firefox. But setup face of Firefox is really cumbersome, really hard to do nobody I know actually use it. Everybody is waiting when it's going to be easier Chrome, easy, Firefox. Personally there is a. I use a Puppeteer Firefox which is a separate unsupported tool which again I mean easy to run with no setup. It does set up automatically for Firefox but ideally I would prefer if it was possible not only with these two browsers but with more browsers. It would be really nice if I could switch form factor and modify what kind of network I want to emulate for my stuff. I would expect that on top of Puppeteer we would have some high level frameworks to do high level UI testing I was talking about right now realistic option is not JavaScript for that but Python another I mean thing. I mean that all these harnesses like Selenium or whatever they do not provide enough events so it's pretty frequent to see oh we clicked this button and now we wait for three seconds. Why you wait for three seconds? Because I mean you know it triggers something on server and we don't know. So I mean it's. It posts stuff to server server response. Can you wait for server response? And after that? Well there is no event for that so we have to guess and sometimes I mean it breaks. I mean timeout is always brittle. What if I mean it took five seconds because I mean server was overloaded the other side of the business, I mean they have all these three seconds, I mean delays in hundreds if not thousands of places. So even if you, if your application is fast, it will be running four hours, just waiting three seconds every single time, I mean to do whatever they do. So I mean all this stuff in my opinion is ripe for better tool in automation and whatnot. Gotcha. That's my opinion. Maybe I'm wrong.
Speaker B:
And the better tooling I'm trying to understand is that do you feel like there are tools on the market that exist that would be better that you're not able to use at your organization? Or do you feel like the overall tooling just isn't supporting the workflow that developers would like?
Speaker A:
The last one, basically. I'm not familiar with commercial applications for that to be honest. But all projects and I used to be, before that I used to be an independent consultant. So I had like multiple projects with different companies per year. Never ever I saw any commercial unit testing or commercial UI testing. Everybody uses the same open source unit testing, the same mocha, the same tab, the same choice, the same, I mean like there are like, I mean some bunch of platforms and frameworks you can use and that's it. In my opinion it's not, not good enough. It's plus, I mean all these projects are really old so it means, I mean they huge. It's difficult to rewrite them. For example, if I want to test IE6 code and E6 code, I mean is directly supported by Node by browsers for a long, long time, I have to transcode it using some of the compilers from JavaScript ES6 to JavaScript ES5 and it will bring me a totally different debugging experience because translating stuff, I mean, you know, debugging you basically debugging different code and even more so for performance. ES5 code now slower than ES6 code. And I'm talking only about node, I'm not talking about browser for browser it's even worse again, I mean transpiling polyfills, some other stuff. It's really hard time when I mean, you know, you open your old HTML, I mean by hand opens in browser and it works or it doesn't, I mean gone problem with that, I mean that immediacy of debugging gun flow has gone and every time, I mean we do anything, it takes forever. Any minor thing is uphill battle. So that's how we see it in.
Speaker B:
Your opinion, what would make the tools better?
Speaker A:
Somebody has to put money into that ultimately. Well, by money I'm saying resources to be honest. So we need people, I mean Smart people who have some ideas how to improve it and do it and ultimately time, ultimately money, obviously, ultimately, I mean other stuff, but I don't think, I mean that major companies who can support open source. And this is open source usually, at least right now. So we're talking about Google, Apple, Facebook, Twitter, IBM, aws, Amazon in general. I mean all these guys, I mean Microsoft, obviously, they either not realizing it yet or maybe they have some internal tools they happy with. Kind of like don't want them to go out because they can lose them in some competitive advantage. I don't really know. But I mean, at some point it should be done. We'll see.
Speaker B:
Well, awesome. We're at the top of the hour and I don't want to take more of your time than I asked for. So I really appreciate everything that you shared with me today and appreciate the time that you gave to talk about web testing.
Speaker A:
Absolutely. Anytime. I mean if you have, I mean, some follow up questions about that or if something I said, I mean, it was unclear, just, you know, pin me, I mean, I'm available. Awesome.
Speaker B:
Well, thank you, I appreciate that.
Speaker A:
Thank you. I enjoy it too. Bye.