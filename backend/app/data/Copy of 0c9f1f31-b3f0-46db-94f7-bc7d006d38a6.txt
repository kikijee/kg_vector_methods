Speaker A:
Researchers contracted out to help with the web MDNA study. And one of the things we're looking at for web developers is that testing has been listed as a top pain point over the last two years. So today we're just hoping to learn a little bit more, especially from web developer.
Speaker B:
Sure.
Speaker A:
So, any questions before we get started?
Speaker B:
No, I don't think so.
Speaker A:
Awesome. Sweet. So as we get started, I'm going to send a link in the chat. So if you open up the chat and then either copy and paste or click and hopefully it gets active and then just let me know when you get there.
Speaker B:
All right.
Speaker A:
Perfect. Looks like we're good to go. So just as a heads up, I'm going to share my screen so that James and Allison can follow along. Let me make sure I select the right window, otherwise things could get really entertaining. Okay, Alex and James, can you give me a thumbs up if you see? Perfect. So today we're going to start off with a little interactive activity. Stephen, what I would like for you to do is to add events to this. And you're going to be able to do that in the top left corner, I believe. So when you click to add an event, it's going to populate in the center of the screen. You can drag it right, left, up, down. And the events I would like for you to add are milestones that you have when you are doing your development process. And you could think any project you'd like, because I know projects can change.
Speaker B:
Should I be thinking about testing in particular, or is this just general development?
Speaker A:
I would like you to show me your development process and if you could include the testing, that's going to be what we talk a little bit more about.
Speaker B:
Okay. Okay. So you want me to go ahead and start adding, adding events?
Speaker A:
Yes, please.
Speaker B:
Okay. See here.
Speaker A:
And if you can talk me through what you're doing, I would love to know.
Speaker B:
Yeah, sure. So I'd start with some sort of discovery process, which would be talking to all the people involved in the project, learning what the requirements are, learning what the limitations are, that kind of thing. Yeah, So I don't. I don't know quite if it's an. If it's a specific event, it's a bit more of a phase, but that might start off with an event. That would be a meeting, talking about those things. Research would be looking at what's available to, you know, what tools are available, what do we need, who's going to work on it? Let's see. I would probably look at some kind of a initial Prototype to get things started. Then there would be some regular sort of phases or I'm just going to call it like a development check in. And this is one that might. There might be several of them. So, you know, if you think of it almost like a cyclical thing where we work on features, we check in with the whole team, I would think. And feel free to. You probably just want me to keep talking, I guess, but if. Steer me in any direction you need me to. Let's see, I've already said prototype, but there's like another word I would use for like. I'll call it like a stuck on terminology here. I'll just call it like a beta release or something which essentially beta release isn't quite the right term, but it would be like the first feature complete thing that we would show to everybody and start testing. And that is where I would bring in. Well, I'll just call it testing in general. But that would include. That might include some, you know, functional tests, unit tests, browser testing, maybe some user testing which would be actually putting human beings in front of it and seeing how it works with them. Some sort of cyclical revisions based on what we've learned. So fixing things and testing again. Then we would probably launch something and there might be a few more, you know, post launch maintenance and things like that. But that would. That's roughly what comes to mind.
Speaker A:
That's awesome. So you kind of listed out several different types of testing that you do. I was wondering if you could give me some more information on those.
Speaker B:
Sure. So I have a. In the web development world I'm more on the front end and the design side of things. So I don't have as a heavy like sort of computer science development background. So when I talk about functional tests versus unit tests or something like that, I'm like just on the verge of my own level of understanding. We've. As a, you know, I work at a web development firm. So when we're mostly developing web apps and mobile apps as part of that process, whenever we're writing code, we're trying to create unit tests for anything that is like an API, something that we can really test reliably with a simple test. Does this thing work at all? And those would be tests that would run every time like a PR is checked in, in something like GitHub workflow. Then we might be. I don't even know if I'm using the terminology right, but I guess maybe it doesn't matter what I mean when I say like a functional test would be A test that is an automated way of running through a whole process. Like having a robot essentially go through the checkout process on a website or something like that. We've used tools like Cypress and Jenkins to automate some of that in the past. Then when I mentioned user testing that I'm talking about, I would sit down with recruitment four, five or six actual human beings and sit down with them one on one and ask them to do some process and sit there and watch them and hope they are able to talk through like you're hoping. I'm talking through things right now. Hopefully they'll verbalize what they're doing. I have a little bit of that. I hope that doesn't disqualify me. So user testing, like watching people maybe recording it and getting their reactions and seeing where they succeed and fail.
Speaker A:
I mentioned, you know, my job.
Speaker B:
I've done little bits of it. I can't say I'm like a professional researcher or anything like that, but I've done little bits of it. I did mention browser testing in there too. And I say browser testing, but I would expand it to just like sort of device testing. So making sure whatever we've built works as intended on all the different clients it might run on that might be a slew of web browsers across different operating systems or mobile devices. Sometimes some tools we use to help us with that, though it tends to be not automated for us, we open it up on the device. If we don't have the device, we use something like browser stack as a tool to get virtual access to devices or browsers we don't have. But we're opening the thing up and trying it with those with different browsers, making sure it works.
Speaker A:
That's awesome. It sounds like you do have a good amount of tests. Are you happy with the amount of testing that you do?
Speaker B:
Not quite, no. At our firm, I would say we're late to the game of discovering the value of testing. And it's taken us a while to figure out what works and what doesn't. But I think we all kind of testing has been a little bit like getting exercise and eating healthy. Like, we all know we want to do more of it and we never do enough of it. So we're all ashamed of how we're falling short. And I'm kind of joking, but I think that's actually a real metaphor. Like we. We know it's valuable, we know it's good for us and we don't do it enough. It's hard to build time for it. Like we, we don't, we don't have the muscle memory to include it in all of our estimates when we're assuming how long something's going to take or how much it's going to cost. We get better at that over time. But to succinctly answer your question, no, we feel like we need to do more. I feel like I need to do more.
Speaker A:
Real quick question. Can I ask about are you like a contractor with your company? How does that work?
Speaker B:
So I'm. The company I work for is a web development company called Silver orange. There's about 20 to 25 people that work there. I'm an employee and a part owner of the company, like a partner in the company and we are sort of a client services firm. So we're building as a company, I'm working as an employee with a team building websites and apps for third party companies and organizations.
Speaker A:
So do you feel like your team is starting to move more in the direction of more testing? And is there any specific testing you're moving towards?
Speaker B:
Definitely moving in that direction. Not so much in terms of specific types of testing, but we're hiring for it. We realized it's something that none of us have the time or skills to, to know what we need to do. So we're actually right now interviewing people trying to hire. Hopefully we'll hire someone in the next couple of weeks as like sort of to lead testing and quality assurance at our company. That would help us figure out what we're missing. I can say one thing that we, we did start doing a type again, I might not be using the right terminology, but we started doing, I forget what the software package is called, but we were doing what were called snapshot tests. And this is like web development with mostly in react based web development where you know that we build a react component, you'd see what it, the output it creates, you would store that and then the test would just make sure it matches the output that you expected. But. And we kind of as amateurs, new people to testing, we went overboard with that and we built a zillion snapshot tests. We thought we were awesome. And then we realized that like anytime we changed anything, we had to change every single test. And we got to be doing that so quickly and automatically that we were barely paying attention to updating the test. So the tests were valuable on paper, but in practice they weren't actually helping us catch valuable or at least the trade off of time investment. It wasn't helping us catch real bugs or real problems. So we kind of were like we need someone who has more experience to help us figure out more valuable ways to test.
Speaker A:
That's actually some great information. Do you guys currently have a test suite that you use?
Speaker B:
Yes, it varies from project to project. If it's okay, I'm actually going to look at just so I get some of the terms right here. When we use GitHub for just source code management and whenever a pull request like a change happens in GitHub, there are some tests that run automatically that happens. Let me see here. Bear with me just for a second. So I'm looking at an example here where we run some. So Jenkins is the platform that's running these tests for us. And it's. This is sort of a combination of sort of tests and QA and deployment is all kind of mixed in together. But when someone makes a change, it's running some linting like making sure that the code is to the style that the team has agreed on. Then there are Selenium based tests that are running that are of sort of it's spotty coverage. Some projects have tons of tests, some have one or two. Some of them are out of date, some of them are not. So they're not in great shape. But there are some tests in Selenium and I think that yeah, that's actually it that's running on Most of our PRs and most of our projects these days.
Speaker A:
So as a whole, are you happy with that framework that you're using that test suite or are there things you wish that you had more access to?
Speaker B:
The. The automation aspect of it that the way I create a pull request in GitHub and Jenkins runs a bunch of tests automatically and I can see the results. That is actually great. That's working quite well for us. We're quite happy with that. The creating new tests and making sure the tests we have are valuable and maintained and up to date? I would say I'm less satisfied with that. I'm not sure how much of that I can put on the software, frankly. Like it's partially just a human and a developer problem that we haven't had the time or energy to put into it.
Speaker A:
No, that's an awesome answer. Just ballpark. And if you don't have an answer, that's perfect as well. Do you know about how many test cases you usually are running?
Speaker B:
Again, this might be hard to to totally quantify, but there would be projects where there are either none or like one or two and the baseline would be just some simple like linting make sure that there's like not like the thing doesn't fail to compile or something like that. And there would be some of our projects that have better test coverage. There might be as many as 20 or 30 tests running on a print. So. And it would really literally go anywhere in between. There'd be. I'd have probably a project that would have everywhere from 1 to 20.
Speaker A:
So I'm going to say let's assume we're in a project where you have some of those tests being run. Do you find that they catch the problems in the application pretty quickly?
Speaker B:
There are classes of problems that they seem to catch very reliably and quite well. It tends to be the things that are simpler to quantify. Like I mentioned linting. I don't even really know if that linting qualifies as testing or not, but that part works well. If somebody doesn't follow the code style and they have the wrong indenting or a class name that doesn't follow the rule set we have or something like that, that does get caught. And that works really well for us. And it'll point out something that I should have known. And great, I've learned from it. I fix it. But when it comes to sort of bigger or sort of almost like more real world bugs, I don't have a great sense of how effective it is. So in a way that's a no. Like it's not that I know it's ineffective, but I don't know how effective it is. So I'm not super confident that it's helping us discover a lot of issues.
Speaker A:
Can you give me an example of a time where it didn't catch something that you. You caught yourself?
Speaker B:
Ah, let me see. Well, the cases tend to be places where different systems interact together. So let's see if I can actually come up with a real example here. Well, an example I can think of recently is we had an issue with the configuration on one of our web servers. There was a security configuration set up wrong. So I think it was like a cors. I forget even what the acronym stands for. Cors. It was something that didn't work in production, but worked in the test environment and worked on our development environment. Our test didn't sufficiently replicate the real world environment. Things look like things were great until we deployed it. And only after we deployed it did we discover that there was a pretty critical bug. In this case, that's a case where in any individual pull request or code change we might have made, there was no explicit bug. It was like once all the pieces got to put together. All the pieces got put together. Only then did a problem reveal itself.
Speaker A:
That sounds exciting. So I'm going to get just a little bit more granular with you, I guess tool wise, do you have a bunch of tools that help support your workflow?
Speaker B:
Yes. Tools coming to testing in particular. Well, I guess tool wise, most of us are working in as our code editor. Our development environment is Visual Studio Code VS Code. Although everyone uses whatever text editor they want. But that's been a popular one that most of us, including myself, are using. There tend to be some good extensions for that that help with things like I mentioned before, like linting. So we have, wherever possible we would set up all the configuration in a project to say this is our code style for JavaScript files, this is our code style for CSS files and we use tabs or spacing for indents and we do use commas or we don't or whatever the specifics might be. The more important thing to us is that we're all doing the same thing so that configuration gets set up in a project and we're using. Well, one tool I guess we'd be using is called Prettier JS that helps with formatting of code and linting. Everybody, including myself, loves Prettier JS for formatting code. Then we're using, as I mentioned, we're using GitHub for source code management, the integration it gives us with Jenkins to run tests. When it comes to actually building tests, we're not using. We're using Selenium to do some tests. Frankly, I don't know a whole lot about how we create tests for Selenium, but as far as I know the process is fairly manual. And one other tool that we do use that comes to mind is Browser Stack, which is a service that lets us use virtual devices so we can test all kinds of different phone types and sizes and browsers and operating systems so we don't have to have 50 phones on everyone's desk.
Speaker A:
I bet that helps.
Speaker B:
Yes. Yep.
Speaker A:
How did you learn about some of these tools?
Speaker B:
Huh? It would prob primarily be people from the team including myself, just sort of following sort of industry news and press. Like there would be just things like blogs of other and social media of other developers that we would follow like a news sites that are sort of tech focused. I would like examples might include like the Hacker news site or Ars Technica. There'd be people in the team that would read those and probably within some of the niches there might be some Sort of more niche news sites around maybe react or just JavaScript development in general or front end development. There's a podcast called the Shop Talk show that several of us listen to that's sort of geared toward front end developers and they would sometimes get into talking about some of these testing workflows. Definitely learned some things from there. The Mozilla, the MDN network is like a resource, common resource for us. I wouldn't say would go there to learn about tools, but like would go there to learn about fundamental web development stuff.
Speaker A:
That's awesome. So how did you end up shaping your testing process?
Speaker B:
This might not be a great answer for your purposes here, but we had a person on staff who put together kind of the way that we've been using tests for a year or two and that person left the company a while ago and we're trying to replace them now. So it was kind of like a, it was more of just like a staffing issue where we had someone thinking about that, dedicated to it. We lost that person on the team and we feel that we need that as a company so we're trying to find someone to replace it. In the absence of that, we have a. I mean there's a role in our company, there's. One of the developers is the technical director at the company and he would sort of steer our requirements and set our requirements for what kind of tests we need to do. And that's. So there's a combination of. We had a dedicated QA person and testing person that helped us establish some of the tests we have. And then some of our existing developers have learned what we could from that person, though they're not with the company anymore and continue. We've kind of just sort of continued with what they helped us start, but haven't been able to improve or add on to it, which is why we're, we're trying to hire again that, that.
Speaker A:
Makes sense for your test, for how you test specifically and how you do yours. Is there any way that you develop your specific, like your ideal. This is how I want to knock it out.
Speaker B:
When I think of testing for myself, I tend to be thinking more of essentially sort of manual testing as opposed to any kind of automated tests. And there I'm thinking of testing for compatibility across devices and browsers. So there we would try, we would establish sort of a set of requirements. So like how far back do we need to go in terms of older web browsers or older phones? Are there specific requirements for. I mean sometimes we work with people in the medical industry and they have hospitals with like weird old enterprise computer systems that can't upgrade to newer versions of Internet Explorer. So we have to support some weird old version of Internet Explorer. Or there might be a project that, where a client is geared towards just demographics, where there might be people that don't have access to the latest computer and phone hardware. So we would set requirements basically what are all the devices we need to make sure we're working on? Sometimes we would build our fancy way of calling it is a test matrix. But essentially it's a spreadsheet of pages or features and devices or browsers. And is it working in all of those things? Often actually this could have gone in my tools list, but sometimes we'll just use a Google Sheet, like Google Sheet spreadsheet and with some carefully crafted automated formatting so things turn red and green as we mark that we've tested a certain feature on a particular device. One challenge there is that it's like that's a very manual process. So if we do it partway through the development process and things continue to change, you kind of have to retest everything. And as it's manual, it's not realistic to retest everything after every change. So it might be something where we do it partway through the development so we turn up any major issues and then soon before we launch. So we make sure we don't launch with anything really broken.
Speaker A:
That's awesome. So you've talked a little bit about browsers and devices. I was curious, what specific browsers do you like to support? And are there any that maybe you're kind of like.
Speaker B:
Yeah, so when it comes to browsers on the desktop, we're primarily thinking about, I mean, Chrome for Windows and Mac as it's very popular. A lot of people use it. Firefox for Windows and Mac, I guess for these two of those two browsers we'd also be keeping Linux in mind. Although if it works in Chrome, there's a pretty good chance it's going to work across the. Or works in Firefox. Probably going to work across all platforms. But Firefox, Chrome, whatever Microsoft is calling their browser currently, that would include there was Internet Explorer 11. Anything before that we've long since disregarded, primarily because it doesn't even support like the basic security requirements for things like E Commerce. But we often do still Support Internet Explorer 11 and then any newer version of Internet Explorer or Edge a Safari on the Mac. We don't really realistically bother with really any other testing significantly in any other browser. Unless there's some unusual special consideration like we're not testing an Opera. Actually, most. We tend to assume that there are several browsers I know that are based on the same rendering engine as Chrome. Like there's Brave and Vivaldi, maybe. I don't even know what they're called. Opera. Now, we don't bother testing on all those individually, partially because not many people are using them when they visit our sites, though that can be a catch 22, and partly because we're assuming if it works in Chrome, it's probably going to work in those as well. So that's on the desktop side. Are you interested in the phone side as well?
Speaker A:
I am actually, because you brought that up like twice now, so I'd love to know.
Speaker B:
It's definitely like, we consider the like phones, like, are definitely a peer to like, as many people are hitting a lot of the things we're building, the web tools we're building on the phone as they are on the desktop. The most important thing for us on phones is like, whatever the default browser on the phone is that people are buying. So that tends to be Mobile Safari on iOS devices and either Chrome on Android phones or whatever variation like Samsung would have. A lot of the Samsung phones seem to have a variation of their own browser that's somehow related to Chrome, but kind of different. So primarily on the phone, we'd be thinking of Safari on iOS, Chrome on Android. It used to be the case where there was also the sort of Android browser before Chrome became the default Android browser. Not testing for that much anymore. Several of us are Firefox for Android users, so it gets tested and we love it, but not. I don't think a lot of our customers are using it, so we're not testing it heavily for that perspective. And then it would be like whatever the browser that shipped on Samsung phones for the last couple of years, the Samsung, I don't know, they call it Internet browser or whatever it's called these days.
Speaker A:
That's. That's awesome. So I have one more question before we hop into the next activity. Is there any testing that you just don't do?
Speaker B:
Hmm, not much that comes to mind. I mean, we're not doing like load testing, like testing systems for like, heavy, heavy use, mostly because we just haven't felt the need for it. No, nothing else that comes to mind.
Speaker A:
Awesome. Tell you what, Stephen, I'm going to have us go back to this graph and I'm going to actually turn on some guides. And when I turn on the guides, you're going to see the middle line is labeled neutral, the top line is labeled frustrating and the bottom is less are not frustrating. So for all of these milestones you marked out, I would love for you to drag them either up or down based on how they make you feel when you're doing it. And if you could walk me through why, I would love to know.
Speaker B:
All right, let's see. Discovery, I'd say is probably on the not frustrating side, frankly, because it's the beginning of the project, it's new and interesting. There are frustrating aspects to it, sometimes nailing down requirements a little hard to get from people. I'd say research is kind of frustrating because choosing the right tools and platforms I'm speaking too vaguely here. More specifically, if we're doing web development, do we use. Should we be using a JavaScript framework? Should it be React or should it be something else? Should there be sort of a meta framework like Create React app we use Sometimes there's an overwhelming number of choices there and choosing the right ones is kind of daunting. The biggest concerns there are like getting stuck on a choosing the wrong one and ending up it being two years later and you're maintaining a project that is written with a totally dead piece of software. So that's like that research is in. Choosing the right mix of technologies is frustrating for that reason, I'd say prototyping is less frustrating. Again, probably is like you're building something quick and there's a interest and satisfaction to that. The developing, developing and checking in, just adding features kind of neutral. No strong feelings there. Getting a beta release out feels great because. Not frustrating because it's. You're actually getting something real in front of front of people and getting feedback that's very satisfying. Testing is kind of frustrating because there are so many variations of browsers and operating systems. I mentioned a few times that we use Browser Stack, which lets us. We love because it gives us access to a bunch of devices, but we also hate it because they're virtual, so they're not quite as fast and responsive as a real device might be. So that can be pretty frustrating. Sometimes we blame the tool there too. But also I think if we build a really complex JavaScript app and we're testing it in Internet Explorer 11 on a virtualized system, IE 11 is slow. So it's frustrating to test that because we've probably written something that isn't well optimized for that. So that whole process can be frustrating. User testing when we do it, it's great and we always learn a lot from it doesn't always get done. That's kind of frustrating sometimes there might not be budget or will from a client for it. Getting feedback and revisions. No strong feelings there. And always feels great to launch something when we're done.
Speaker A:
Awesome. That's fantastic. And I feel like we've talked kind of agnosium about testing which is good. Right. Is there anything else you want to touch on about why it makes it.
Speaker B:
So frustrating around testing? A little bit repeating myself but the volume of different variations of browsers, browser engines and operating systems is a lot to test for. That has gotten much better. So the like we tend to think of the sort of evergreen browsers which is the browsers that are kept up to date pretty well by the operating system so people don't have to upgrade manually that I would include Chrome and Firefox and modern versions of Internet Explorers Edge. But older versions of IE and Edge. I don't know about Edge. Older versions of IE people tend to be stuck on them. That's super frustrating and holds things back and just knowing how what the right tools to build good tests in terms of automated tests even what's possible there that I find kind of frustrating because it's just something I just don't know enough about and it's a bit overwhelming. I feel like there's more we could do but I just don't know what it is.
Speaker A:
I guess. Do you. Do you search out more information on automated testing.
Speaker B:
Myself in my own role day to day? No. Except I mean the closest thing to that would be in my capacity as a partner at this company would say we need to hire someone who can help us figure this out. So that's kind of like a way to learn more about it because it acknowledges that we need to know more about it. But it's not something I'm researching myself. There would be other people at my company that do do that. So that's not to say it doesn't happen at all. It's more an a side effect of my particular skill set just isn't oriented toward automated testing.
Speaker A:
Those are some awesome answers. Thank you for that. Sure. So this next part is kind of a game called Rapid Fire. You've already talked a lot about a lot of what I typically ask about. So if you've heard of the term and you feel like you can give me a little bit more information, I would love to know what you can. If you haven't heard of it, just let me know and we can skip on.
Speaker B:
Sure.
Speaker A:
So we've talked about cross browser testing and of itself. Have you ever heard of a tool called Playwright?
Speaker B:
Yes, if it's what I think it is. I think it might be somehow related. An open source tool from Microsoft. I might be conflating it with something else that lets you write JavaScript tests, kind of, sort of. That's as much as I know about it or don't.
Speaker A:
No, that's a perfect answer from there. Have you ever heard of integration testing?
Speaker B:
Yes, but couldn't give you a good clear definition of how integration testing differs from functional or unit tests. I'm just repeating testing terminology I've heard in the past. Integration tests I think is like tests that test something more than just a single function, like going through a whole process. But my confidence in my answer is low.
Speaker A:
Okay, I like your answer though. It's awesome. What about end to end testing?
Speaker B:
Well, that makes me less confident in my previous answer because that's kind of what I was thinking I was describing about integration testing. I would think end to end testing describes something that would test the whole software stack through a process. Like an example I brought up earlier, having an automated system hit a web app that we've built and go through a signup process or a checkout process, fill out forms, see if they give the right responses and actually run through the whole thing and see if it succeeds.
Speaker A:
What about the tool of Cypress? Have you ever heard about this?
Speaker B:
Yes, Cypress has definitely come up in conversations about testing at our company. We may even have used it before. But I couldn't tell you. I couldn't differentiate it from any other testing tool.
Speaker A:
Thank you. What about performance testing?
Speaker B:
We have used tools in the past. There was one called GT Metrics that would do regular performance testing on a site. We haven't used that in quite a while. These days we would rely on some of the tools built into the development tools built into the browsers, like the Firefox dev tools. Just things like there's like the network panel telling us how long things are taking or the profiling panel. Also use some of the Google tools like again, whatever they call it these days, Google Page speed or Google Lighthouse. I think I'm using different terms that all describe kind of the same toolset where Microsoft or Microsoft Google will give a bunch of performance metrics for the web, including, you know, they've got a whole new set of kind of buzzwordy ones that I think are actually kind of helpful around. Why do they call them core web vitals? Hard to keep up with the terminology, but so yes, performance testing, definitely something we keep in mind tends to be something we do manually during testing, not something we've tended to automate.
Speaker A:
Is there a reason why?
Speaker B:
No, not a good reason. I'm not sure if we've found tools that make it easy for us to do, but I know that we have talked about in the past, like the idea of, for example, bundle size, like having something tell us when we make a pull request on GitHub that before this pull request, people were going to download 278 kilobytes of JavaScript and now they're going to download twice that or something like that to help us keep a closer eye on it. But we don't have anything like that in place right now. We'd like to. I'd like to.
Speaker A:
Have you heard of Lighthouse?
Speaker B:
Lighthouse? Is Lighthouse one of the Google things I think I might be conflating again, I think when you say Lighthouse, I think of the Google performance testing tool, and if that's what it is, then I use it. I just don't remember if that's the exact name for it. They clearly have a branding problem there.
Speaker A:
You're doing a great job. I don't think we've talked about accessibility testing yet. Is that something you guys do?
Speaker B:
Yeah, not in an automated manner. This is something that we were probably negligent on for a long time and have come to understand in the last couple of years that it's more important. We've been using a this just this year, actually, we've started using a checklist, like just a manual checklist from a project called the Accessibility Project. It's like the a11yproject.com or.something. but the accessibility Project, I think it's like a community project that has a really great checklist about. Just like here's what you need to make sure is accessible on your website or web app. We've used that. We're actually in the process of boiling it down into a checklist that we can actually use when we start a project, before we launch a project, when major changes happen in a project. We've also been finding that the development tools built into the browsers have been getting better at helping us test accessibility things. Firefox, DevTools in particular. Now they make it easy to check tab order for keyboard accessibility, color contrast. We do test for color contrast, minimum color contrast for text and icons. Not automatically like at the beginning of a design process. We'd look at some of the key places we're using it and then maybe at the end we'd test again.
Speaker A:
Have you heard of a tool called Web Hint?
Speaker B:
No. No, I have not.
Speaker A:
Good, good answer. Component testing.
Speaker B:
I don't know. I don't really know the term of component testing. I mean, we develop with React web apps where we're developing components, and I would presume it would mean testing on a component level, but that's. I just be. It's not a term I've used myself.
Speaker A:
I believe we've chatted about the other test that I had questions on. So before I ask my next question, I'm going to turn it over to James and Allison. Do either of you have any questions for Stephen?
Speaker C:
I have a couple. Stephen, you mentioned at the very beginning of the interview some of the testing that you're doing and the time that it takes to do those tests and maybe not being valuable enough. And I'm wondering if you could just help me understand what would valuable be to you? Like, how would you measure the value that that test would provide?
Speaker B:
Hmm. I would say something that is catching real world issues. Like if there's something that, like I mentioned before, we had like a security issue that didn't show up in our testing on our development environment, but did happen in production. So testing that would like, would catch real world bugs. That's like one way. Like if it's catching things, notifying us of them and we didn't know about them, otherwise that would be a real value. Yeah, I think that's kind of all I have.
Speaker C:
Okay. And then one other question. You're looking for somebody to join your organization who might help build out a more consistent process for testing. How did you or the organization put together that job description or like, what types of skill sets are you looking for and how did you arrive at. At those?
Speaker B:
That's a good question. And I was involved enough in that process to at least know about it tangentially. So the tech lead in our company worked with all of the developers to kind of figure out. It's kind of like interviewed the developers internally to figure out, what do you think we need? What can we do ourselves? Where do we need more expertise externally? And what we ended up concluding was that we're comfortable and capable as a team and as individual developers in creating and writing tests. But we need someone who can have the time to be dedicated to think about testing in general, whereas all the developers right now, it's an afterthought essentially, or it's a small piece of the zillion other things they need to do in a day. So just literally Just having someone who has more headspace and time to think about testing is valuable. And in terms of some of the specific requirements we've looked for, we've been looking for people who have some development experience themselves so they can understand and work with the developers to help them develop tests that are valuable. And then I wouldn't have a good list of all the specific technologies. But frankly, we weren't too concerned about people who've worked with Cypress or whatever, the Selenium or whatever testing package. It's more that someone has kind of a well rounded experience and knows.
Speaker D:
What.
Speaker B:
Types of tools can help us figure out which tools would actually fit with what we're doing.
Speaker C:
Awesome. Thank you. Those are my questions.
Speaker B:
Sure.
Speaker A:
James, do you have any questions?
Speaker D:
So I guess should ask one. So one thing I was wondering is like the chart here kind of stops at launch. Like you've talked about maintaining sites. So like how often do you run into issues like bugs in production after a site has launched? Either like because the site changes or browsers change or, you know, whatever.
Speaker B:
So that's a really good question and actually kind of reveals kind of a flaw in my, in my workflow here because probably 80% of the work we're doing is adding to or maintaining existing sites or apps. But actually developing a brand new thing from scratch is sort of a much smaller part of that. That said, it tends to be we might be adding a new feature set or a new big feature and that workflow applies to building that feature onto an existing site. But we do definitely we would get feedback from our clients or from the support teams at some of our clients that people are running into this bug or this isn't behaving the way we expected. And that would come back to our team for debugging. And then we would have to figure out, is it a browser problem, is it a bug we introduced ourselves, is it some sort of misunderstanding? So the development process kind of in the middle of my testing workflow, there's that development check in which I was kind of thinking of as like a cycle of developing and reviewing, developing and reviewing. We almost come back to that as a steady state after launch. And there would be ongoing testing there, but not as much as we'd like. I think after launch we're more apt, I think, to forget about checking in and making sure that new features we've added work with all the browsers or work with all the accessibility tools.
Speaker A:
Cool.
Speaker D:
And then I guess one other thing I was thinking about was like you said, you test against really Wide range of browsers. Is there a specific set of issues that you're looking out for? Like areas where, you know, this is the kind of thing where we run into cross browser issues.
Speaker B:
Yeah. So on the layout and CSS side of things, there would be sort of like a subset of CSS where we would expect to run into issues in like Internet Explorer 11, for example. So that would tend to be around things that are newer to css, things that have been around for years. But like, you know, something like using Flex and flexbox is in my mind now actually fairly stable. But in new browsers we know it's going to work well, but it was pretty new when Internet Explorer 11 was developed, so it's fairly buggy there. So this almost like a known set of common flexbox display bugs in Internet Explorer and in Safari that we would go to. So when we're doing testing there, we would not at all be surprised to say, okay, we're going to go look at the site and in IE11, oh my God, it looks terrible. But don't worry, we run through our checklist of common flexbox or more recently CSS grid bugs, and it tends to be that once you fix the top five common bugs, you're usually in pretty good shape there. So CSS layout around newer types of CSS like to a lesser degree flexbox because it's not as new and then grid, which is newer, so definitely expect more compatibility issues there. Similar on the side of the JavaScript side of things, it tends to be more performance related. And by performance I almost mean like something in Internet Explorer 6 being slow enough that it just kind of won't work if we have too much poorly optimized code. A new modern browser might be nice to us and just churn through it. An older browser with a less efficient JavaScript engine like Internet Explorer 11 is more likely to choke on it.
Speaker A:
Cool. Thanks.
Speaker B:
Sure.
Speaker A:
So, I had one question before I turn it over to you. You've talked a lot about how you prefer or you do a lot more manual testing. I guess my question is, do you see yourself maybe doing more automated testing in the future? Future.
Speaker B:
Yes. Partially because I hope our organization sort of essentially brings that to me as a feature. So where I'm not setting up the infrastructure for automated testing, but Smart QA lead has developed a bunch of automated tests and whenever I write code and submit it, it's going to get run through that gauntlet. In a way I'm like offloading that desire and that work to somebody else. I do Expect and hope that more tools will evolve that make it easier to do more types of automated testing like Browser Stack I've mentioned a few times, didn't exist however many years ago, and now it does, new tools will come along that will make it easier for us to maybe test to make sure a website is working appropriately in a whole bunch of different browsers every day or every time there's a change checked in. But that would be sort of dependent on the tools and I'd adopt them if they were good and easy and affordable.
Speaker A:
So for you, what is easy and affordable and good meaning?
Speaker B:
Good means enough, they're easy enough to use that I actually use them. So there's not enough of a usability or technical hurdle to understanding the tool. Affordable isn't as I guess this would vary wildly. So I was going to say it doesn't matter a whole lot. We're a mid sized company dealing with small to mid sized clients. So you know, if we, if we're paying for services, you know, some of them are $10 a month, some of them are $1,000 a month. But I'm thinking of services that tend to be in the range of hundreds of dollars or less per month. I realize it's a big range, but it's, I'm not thinking of like enterprise software that's going to cost us a $500,000 license or something like that. It tends to be. I mean I keep using Browser Stack as an example that's like a per user license. And frankly I don't even know off the top of my head what we pay. But it would be, you know, 10 or $20 per month per person or something like that. Awesome. That was the good and the cheap. I forget what the third one was.
Speaker A:
You said good, easy and cheap and I think you already answered easy is something that.
Speaker B:
Yeah, yeah, good and easy kind of overlap there.
Speaker A:
So with the last couple minutes, just with respect to time, I wanted to turn the floor over to you Steven, and any comments, questions, anything you want us to bring up to Mozilla, now is your chance.
Speaker B:
Sure. The developer tools in Firefox have been great and have been very useful to us in debugging and development, so. And they keep getting better. So that's awesome. The developer tools and all the browsers are pretty good these days, but feels a little bit like there's like a. Browsers don't haven't had a lot of visible innovation, but the development tools still continue to get better and leapfrog each other and it's awesome. So that's great but like, the more the better. If they keep getting better, we're going to keep using them. The MDN site in terms of documentation I don't like. All of us on the team use a lot as a reference and find that very valuable. I think one thing that I think web developers could use help at is understanding and almost like there's like an evangelism aspect of it, understanding accessibility and accessibility testing and helping people understand why it's important and how it's valuable and how it's. How it's necessary. Because I think that was something where we felt like we were experts for a long time but didn't have a great understanding of and have only started to improve that. There are good. We have found good resources there from like some of the, like the Mozilla hacks blog and mdn. And though I don't think that they're at Mozilla right now, there was Jen Simmons was a developer who produced a bunch of materials on YouTube that we would. That were really helpful to our, to our team. So all of that stuff is great. Keep that coming.
Speaker A:
Awesome. Alison James, anything before we thank Stephen for his time. Awesome. Well, Stephen, you did awesome. So thank you so much for your time today. Yes, I'm like full high fiving for you. Anything else before we head out?
Speaker B:
No, I hope it was helpful and thanks for taking feedback and yeah, glad to give some.
Speaker A:
Awesome. Thank you so much, Steven. Have a great day.
Speaker B:
You too. Thank you very much.
Speaker A:
Thank you.
Speaker D:
That was really helpful.
Speaker B:
Yeah, sure.
Speaker A:
That felt good.
Speaker C:
You want to stop the recording?
Speaker A:
Yeah. Thank you. I was like, there's someone I'm forgetting.